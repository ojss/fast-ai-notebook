{"cells":[{"cell_type":"markdown","source":"# AWD LSTM","metadata":{"tags":[],"cell_id":"3a9fb304-ce83-4b67-9abf-9ed59b3e1d89"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"adb6bb7c-970d-48a8-8270-fac359adab00"},"source":"!pip install git+https://github.com/NVIDIA/apex","execution_count":1,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/NVIDIA/apex\n  Cloning https://github.com/NVIDIA/apex to /tmp/pip-req-build-xg_zth_f\n  Running command git clone -q https://github.com/NVIDIA/apex /tmp/pip-req-build-xg_zth_f\n  Running command git submodule update --init --recursive -q\nRequirement already satisfied (use --upgrade to upgrade): apex==0.1 from git+https://github.com/NVIDIA/apex in /opt/venv/lib/python3.7/site-packages\nBuilding wheels for collected packages: apex\n  Building wheel for apex (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for apex: filename=apex-0.1-py3-none-any.whl size=186403 sha256=35e9cddc095033a21aa47495676b5d145926e2f502a142a99130790a2dfe4aec\n  Stored in directory: /tmp/pip-ephem-wheel-cache-kl_botqq/wheels/dd/7b/dc/dc522332f3f6f60db5440cbcc4ee70aa155c2cf7d1f15b6900\nSuccessfully built apex\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"02bf4e80-becd-428d-89b7-12c650992ff3"},"source":"# export\nfrom exp.nb_12 import *","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"67aa55d2-4b63-427c-b0f9-22975ec1607f"},"source":"%reload_ext autoreload\n%autoreload 2\n\n%matplotlib inline","execution_count":3,"outputs":[]},{"cell_type":"code","source":"!ls /datasets/fast-ai-nlp","metadata":{"tags":[],"cell_id":"0bd7d10e-0b2b-4fab-a2e5-1505fd2cd374"},"outputs":[{"name":"stdout","text":"ag_news_csv.tgz\t\t\tsogou_news_csv.tgz\r\namazon_review_full_csv.tgz\twikitext-103.tgz\r\namazon_review_polarity_csv.tgz\twikitext-2.tgz\r\ndbpedia_csv.tgz\t\t\tyahoo_answers_csv.tgz\r\ngiga-fren.tgz\t\t\tyelp_review_full_csv.tgz\r\nimdb.tgz\t\t\tyelp_review_polarity_csv.tgz\r\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Data","metadata":{"tags":[],"cell_id":"e029210c-8e56-4562-8d64-ecf61ed5ab68"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"2ea099d0-bd19-4c36-a2bf-aa1a4fa59a79"},"source":"path = datasets.untar_data(datasets.URLs.IMDB)","execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"We have to preprocess the data again to pickle it because if we try to load the previous `SplitLabeledData` with pickle, it will complain some of the functions aren't in main.","metadata":{"tags":[],"cell_id":"b7a9977a-0d78-473f-986b-3c0aacbbe3ef"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"b3607576-6d78-4489-9a41-b3c01d5a101b"},"source":"il = TextList.from_files(path, include=['train', 'test', 'unsup'])\nsd = SplitData.split_by_func(il, partial(random_splitter, p_valid=0.1))","execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"86f177fd-aac4-41d0-8134-2392d1d3c184"},"source":"proc_tok, proc_num = TokenizeProcessor(max_workers=8), NumericalizeProcessor()","execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"58360311-6bfb-4a8b-9e68-bc8fd1b62f49"},"source":"ll = label_by_func(sd, lambda x: 0, proc_x = [proc_tok,proc_num])","execution_count":8,"outputs":[{"data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='0' class='' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      \n    </div>\n    "},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='0' class='' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      \n    </div>\n    "},"metadata":{},"output_type":"display_data"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"cb5ac430-07d0-4db6-8c9f-7007ff53151a"},"source":"pickle.dump(ll, open(path/'ll_lm.pkl', 'wb'))\npickle.dump(proc_num.vocab, open(path/'vocab_lm.pkl', 'wb'))","execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"2dbc807e-24b4-4cb3-a854-7e50dd4da4e3"},"source":"ll = pickle.load(open(path/'ll_lm.pkl', 'rb'))\nvocab = pickle.load(open(path/'vocab_lm.pkl', 'rb'))","execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"8502a086-3c06-47c6-a3da-7e884adf9629"},"source":"bs, bptt = 64, 70\ndata = lm_databunchify","execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## AWD-LSTM","metadata":{"tags":[],"cell_id":"8a05e0b1-e9f6-41e3-b8c1-340baf6f8f73"}},{"cell_type":"markdown","source":"Before explaining what an AWD LSTM is, we need to start with an LSTM. RNNs were covered in part 1, if you need a refresher, there is a great visualization of them on [this website](http://joshvarty.github.io/VisualizingRNNs/).","metadata":{"tags":[],"cell_id":"d0f78ec0-3837-4c26-95e3-aa9d729e75ba"}},{"cell_type":"markdown","source":"[Jump_to lesson 12 video](https://course.fast.ai/videos/?lesson=12&t=6330)","metadata":{"tags":[],"cell_id":"a70ce2c2-4aa6-4886-9761-611f94abc9bf"}},{"cell_type":"markdown","source":"### LSTM from scratch","metadata":{"tags":[],"cell_id":"5db9cef0-c77f-4cd9-a2de-d3e216437d63"}},{"cell_type":"markdown","source":"We need to implement those equations (where $\\sigma$ stands for sigmoid):\n\n![LSTM cell and equations](images/lstm.jpg)\n(picture from [Understanding LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Chris Olah.)\n\nIf we want to take advantage of our GPU, it's better to do one big matrix multiplication than four smaller ones. So we compute the values of the four gates all at once. Since there is a matrix multiplication and a bias, we use `nn.Linear` to do it.\n\nWe need two linear layers: one for the input and one for the hidden state.","metadata":{"tags":[],"cell_id":"e1c4b47a-c4ec-4873-a73d-a772c04331a5"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"6e442861-1f5f-43b3-bac5-6db300d973a4"},"source":"class LSTMCell(nn.Module):\n\n    def __init__(self, ni, nh):\n        super().__init__()\n        self.ih = nn.Linear(ni, 4 * nh)\n        self.hh = nn.Linear(nh, 4 * nh)\n    def forward(self, inp, state):\n        h, c = state\n        # one big multiplication for all the gates is better than 4 small ones\n        gates = (self.ih(inp) + self.hh(inp)).chunk(4, 1)\n        ingate, forgetgate, outgate = map(torch.sigmoid, gates[:3])\n        cellgate = gates[3].tanh()\n        c = (forgetgate * c) + (ingate * cellgate)\n        h = outgate * c.tanh()\n        return h, (h, c)","execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"9d9fddbb-5cc7-4f97-bc43-fd5c311cd96f"},"source":"class LSTMLayer(nn.Module):\n    def __init__(self, cell, *cell_args):\n        super().__init__()\n        self.cell = cell(*cell_args)\n    def forward(self, inp, state):\n        inputs = inp.unbind(1)\n        outputs = []\n        for i in range(len(inputs)):\n            out, state = self.cell(inputs[i], state)\n            outputs += [out]\n        return torch.stack(outputs, dim=1), state\n        ","execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"c62c4ace-211e-48ea-b57d-0c05c948f4a1"},"source":"lstm = LSTMLayer(LSTMCell, 300, 300)","execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"58e22198-eacb-4878-bbe0-3d153a2fe7ef"},"source":"x = torch.randn(64, 70, 300)\nh = (torch.zeros(64, 300),torch.zeros(64, 300))","execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"#### CPU","metadata":{"tags":[],"cell_id":"e1c6bebe-98a8-43c0-8aec-e393f9fe7c47"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00a05432-3d85-4e9f-ab80-0fa839b5c376"},"source":"%timeit -n 10 y,h1 = lstm(x,h)","execution_count":16,"outputs":[{"name":"stdout","text":"116 ms ± 8.16 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"c4739b39-e5a0-457a-bc55-c443fccc1cdf"},"source":"lstm = lstm.cuda()\nx = x.cuda()\nh = (h[0].cuda(), h[1].cuda())","execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"2a06c275-1edf-47b7-83d3-bc676ad85090"},"source":"def time_fn(f):\n    f()\n    torch.cuda.synchronize()","execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"#### CUDA","metadata":{"tags":[],"cell_id":"19115c68-1d16-4358-a0f8-4529379b54dc"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"ab775bbc-a237-439a-bc41-44b8a9579f3d"},"source":"f = partial(lstm,x,h)\ntime_fn(f)","execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"6318663b-f6e1-48cc-abd9-43f8d9af3c46"},"source":"%timeit -n 10 time_fn(f)","execution_count":20,"outputs":[{"name":"stdout","text":"25.4 ms ± 545 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Built-in version","metadata":{"tags":[],"cell_id":"2fe660a9-797b-48b6-a8d3-ad1468e254db"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"4a37129c-7281-4086-9c81-59bc8139b995"},"source":"lstm = nn.LSTM(300, 300, 1, batch_first=True)","execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"8f32dea8-7f89-4749-84db-f754ba235c31"},"source":"x = torch.randn(64, 70, 300)\nh = (torch.zeros(1, 64, 300),torch.zeros(1, 64, 300))","execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"#### CPU","metadata":{"tags":[],"cell_id":"990f3b09-e5b4-4d2b-b5a4-0ca734eefc50"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"0060ac0b-1a1c-4f8b-a7c7-a2cbe3464372"},"source":"%timeit -n 10 y,h1 = lstm(x,h)","execution_count":23,"outputs":[{"name":"stdout","text":"92.5 ms ± 2.29 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### GPU","metadata":{"tags":[],"cell_id":"f90dc90f-7254-4cfa-b319-8e36837cf371"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"97fe8a23-5a59-49cd-87da-42f952e77847"},"source":"lstm = lstm.cuda()\nx = x.cuda()\nh = (h[0].cuda(), h[1].cuda())","execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"8b7e63cc-5472-4693-9a7d-b15d9e757f40"},"source":"f = partial(lstm,x,h)\ntime_fn(f)","execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"efe152d7-efad-498e-b482-97376966847d"},"source":"%timeit -n 10 time_fn(f)","execution_count":26,"outputs":[{"name":"stdout","text":"4.71 ms ± 618 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Jit version","metadata":{"tags":[],"cell_id":"bbbb0ce7-70a1-4f4e-ac29-e08f92ef2916"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"310d0e9f-51e1-4c72-8009-03e3b7e1c41d"},"source":"import torch.jit as jit\nfrom torch import Tensor","execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"ce65a32f-14c6-463d-92e2-fb53e3644081"},"source":"class LSTMCell(jit.ScriptModule):\n    def __init__(self, ni, nh):\n        super().__init__()\n        self.ni = ni\n        self.nh = nh\n        self.w_ih = nn.Parameter(torch.randn(4 * nh, ni))\n        self.w_hh = nn.Parameter(torch.randn(4 * nh, nh))\n        self.bias_ih = nn.Parameter(torch.randn(4 * nh))\n        self.bias_hh = nn.Parameter(torch.randn(4 * nh))\n\n    @jit.script_method\n    def forward(self, input:Tensor, state:Tuple[Tensor, Tensor])->Tuple[Tensor, Tuple[Tensor, Tensor]]:\n        hx, cx = state\n        gates = (input @ self.w_ih.t() + self.bias_ih +\n                 hx @ self.w_hh.t() + self.bias_hh)\n        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n\n        ingate = torch.sigmoid(ingate)\n        forgetgate = torch.sigmoid(forgetgate)\n        cellgate = torch.tanh(cellgate)\n        outgate = torch.sigmoid(outgate)\n\n        cy = (forgetgate * cx) + (ingate * cellgate)\n        hy = outgate * torch.tanh(cy)\n\n        return hy, (hy, cy)","execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"42c3937f-1281-4ad0-b816-da98e77155d2"},"source":"class LSTMLayer(jit.ScriptModule):\n    def __init__(self, cell, *cell_args):\n        super().__init__()\n        self.cell = cell(*cell_args)\n\n    @jit.script_method\n    def forward(self, input:Tensor, state:Tuple[Tensor, Tensor])->Tuple[Tensor, Tuple[Tensor, Tensor]]:\n        inputs = input.unbind(1)\n        outputs = []\n        for i in range(len(inputs)):\n            out, state = self.cell(inputs[i], state)\n            outputs += [out]\n        return torch.stack(outputs, dim=1), state","execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"235f2f9d-4ef3-444c-bc85-eacd7f4e54af"},"source":"lstm = LSTMLayer(LSTMCell, 300, 300)","execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"e2acae45-674b-4a36-bf5a-aa44c6c0da69"},"source":"x = torch.randn(64, 70, 300)\nh = (torch.zeros(64, 300),torch.zeros(64, 300))","execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"0f5522c9-8e6a-42d8-9086-77deb57d7155"},"source":"%timeit -n 10 y,h1 = lstm(x,h)","execution_count":32,"outputs":[{"name":"stdout","text":"107 ms ± 12.9 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"23231cd1-7d46-4440-97f1-fa1028bc05e0"},"source":"lstm = lstm.cuda()\nx = x.cuda()\nh = (h[0].cuda(), h[1].cuda())","execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"1aa2c749-ae38-4ee3-8c00-fe42cf22b433"},"source":"f = partial(lstm,x,h)\ntime_fn(f)","execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"2a87341d-6a4f-4209-8b02-5bca8785b565"},"source":"%timeit -n 10 time_fn(f)","execution_count":35,"outputs":[{"name":"stdout","text":"19.2 ms ± 363 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Dropout","metadata":{"tags":[],"cell_id":"6df0f990-0c6c-4338-b753-6454a0d7f1bd"}},{"cell_type":"markdown","source":"We want to use the AWD-LSTM from [Stephen Merity et al.](https://arxiv.org/abs/1708.02182). First, we'll need all different kinds of dropouts. Dropout consists into replacing some coefficients by 0 with probability p. To ensure that the average of the weights remains constant, we apply a correction to the weights that aren't nullified of a factor `1/(1-p)` (think of what happens to the activations if you want to figure out why!)\n\nWe usually apply dropout by drawing a mask that tells us which elements to nullify or not:","metadata":{"tags":[],"cell_id":"7e1895cc-773f-40bf-a88e-280230ed0b81"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"085cff2a-5833-440f-8937-98a962037b08"},"source":"#export\ndef dropout_mask(x, sz, p):\n    return x.new(*sz).bernoulli_(1 - p).div(1 - p)","execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"337d45cf-eae5-49c6-962d-f1e50a7e6ad1"},"source":"x = torch.randn(10,10)\nmask = dropout_mask(x, (10,10), 0.5); mask","execution_count":37,"outputs":[{"output_type":"execute_result","execution_count":37,"data":{"text/plain":"tensor([[0., 2., 2., 0., 0., 0., 2., 0., 2., 2.],\n        [0., 2., 0., 2., 2., 2., 2., 2., 0., 2.],\n        [0., 2., 2., 2., 2., 2., 0., 0., 2., 2.],\n        [2., 2., 2., 0., 2., 0., 2., 2., 2., 2.],\n        [2., 2., 2., 2., 2., 2., 2., 2., 0., 0.],\n        [2., 2., 2., 0., 2., 0., 0., 0., 2., 0.],\n        [2., 2., 0., 0., 2., 0., 0., 2., 0., 2.],\n        [0., 0., 2., 0., 0., 0., 2., 0., 0., 2.],\n        [0., 0., 2., 0., 2., 2., 0., 2., 2., 0.],\n        [0., 0., 2., 0., 2., 2., 2., 2., 0., 2.]])"},"metadata":{}}]},{"cell_type":"markdown","source":"Once with have a dropout mask `mask`, applying the dropout to `x` is simply done by `x = x * mask`. We create our own dropout mask and don't rely on pytorch dropout because we do not want to nullify all the coefficients randomly: on the sequence dimension, we will want to have always replace the same positions by zero along the seq_len dimension.","metadata":{"tags":[],"cell_id":"63ccc048-bade-47ed-8afa-a74af381f3e4"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"371818b5-271b-4a3a-8092-d85dc3291a97"},"source":"(x*mask).std(),x.std()","execution_count":38,"outputs":[{"output_type":"execute_result","execution_count":38,"data":{"text/plain":"(tensor(1.6396), tensor(1.0133))"},"metadata":{}}]},{"cell_type":"markdown","source":"Inside a RNN, a tensor x will have three dimensions: bs, seq_len, vocab_size.  Recall that we want to consistently apply the dropout mask across the seq_len dimension, therefore, we create a dropout mask for the first and third dimension and broadcast it to the seq_len dimension.","metadata":{"tags":[],"cell_id":"feb4633f-5bfb-4284-b3fc-c4609946353d"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"ab74a9df-777d-4049-8b7f-16ae31034e7b"},"source":"#export\nclass RNNDropout(nn.Module):\n    def __init__(self, p=0.5):\n        super().__init__()\n        self.p = p\n    def forward(self, xb):\n        if not self.training or self.p == 0.:\n            return xb\n        m = dropout_mask(xb.data, (xb.size(0), 1, xb.size(2)), self.p)\n        return xb * m","execution_count":50,"outputs":[]},{"cell_type":"code","source":"dp = RNNDropout(.3)\ntest_input = torch.randn(3, 3, 7)\ntest_input, dp(test_input)","metadata":{"tags":[],"cell_id":"5e6bbd83-aadc-4a5b-9bba-e3afc127072d"},"outputs":[{"output_type":"execute_result","execution_count":51,"data":{"text/plain":"(tensor([[[-0.6271, -1.1787, -1.2264, -1.3855,  0.4905,  0.5311,  1.0003],\n          [ 0.2630,  0.1117, -0.3415,  0.3866,  0.4288,  0.2056, -0.3860],\n          [ 0.3656, -0.9671,  0.4248, -0.6932, -0.2435,  0.0457,  0.1831]],\n \n         [[ 0.9614, -0.5359, -0.5053,  0.6134,  0.9013,  0.1548, -1.0373],\n          [ 0.1242, -1.2375,  1.4367, -0.0171, -0.1743, -0.2439,  0.6390],\n          [-1.5063,  0.4196,  0.5764,  0.5497, -0.1499,  1.5334, -1.7988]],\n \n         [[-0.0188, -0.7649, -0.7905, -1.1936, -0.0619,  0.2954, -0.4649],\n          [-0.4612, -0.3387, -0.6082,  1.1636, -1.6281, -0.6541, -0.9733],\n          [ 0.8226,  0.6989, -1.1267, -0.3920,  0.5949,  1.9333,  1.8637]]]),\n tensor([[[-0.8958, -1.6838, -1.7521, -1.9793,  0.7007,  0.7587,  1.4291],\n          [ 0.3757,  0.1595, -0.4879,  0.5523,  0.6125,  0.2937, -0.5514],\n          [ 0.5222, -1.3815,  0.6069, -0.9903, -0.3479,  0.0653,  0.2616]],\n \n         [[ 1.3735, -0.7656, -0.7218,  0.8763,  1.2875,  0.0000, -1.4819],\n          [ 0.1774, -1.7679,  2.0525, -0.0244, -0.2490, -0.0000,  0.9129],\n          [-2.1519,  0.5995,  0.8234,  0.7853, -0.2141,  0.0000, -2.5697]],\n \n         [[-0.0269, -1.0927, -0.0000, -1.7051, -0.0000,  0.4220, -0.6641],\n          [-0.6588, -0.4838, -0.0000,  1.6623, -0.0000, -0.9345, -1.3905],\n          [ 1.1751,  0.9984, -0.0000, -0.5600,  0.0000,  2.7619,  2.6624]]]))"},"metadata":{}}],"execution_count":51},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"9bbd69e9-16c1-455f-a9b6-606204c7cc8d"},"source":"WeightDropout is dropout applied to the weights of the inner LSTM hidden to hidden matrix. This is a little hacky if we want to preserve the CuDNN speed and not reimplement the cell from scratch. We add a parameter that will contain the raw weights, and we replace the weight matrix in the LSTM at the beginning of the forward pass."},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torch.nn.functional as F","metadata":{"tags":[],"cell_id":"dd72aa66-3299-46e2-8f26-2d48c350937a"},"outputs":[],"execution_count":65},{"cell_type":"code","source":"#export\nimport warnings\n\nWEIGHT_HH = 'weight_hh_l0'\n\nclass WeightDropout(nn.Module):\n    def __init__(self, module, weight_p=[0.], layer_names=[WEIGHT_HH]):\n        super().__init__()\n        self.module,self.weight_p,self.layer_names = module,weight_p,layer_names\n        for layer in self.layer_names:\n            #Makes a copy of the weights of the selected layers.\n            w = getattr(self.module, layer)\n            self.register_parameter(f'{layer}_raw', nn.Parameter(w.data))\n            self.module._parameters[layer] = F.dropout(w, p=self.weight_p, training=False)\n\n    def _setweights(self):\n        for layer in self.layer_names:\n            raw_w = getattr(self, f'{layer}_raw')\n            self.module._parameters[layer] = F.dropout(raw_w, p=self.weight_p, training=self.training)\n\n    def forward(self, *args):\n        self._setweights()\n        with warnings.catch_warnings():\n            #To avoid the warning that comes because the weights aren't flattened.\n            warnings.simplefilter(\"ignore\")\n            return self.module.forward(*args)","metadata":{"tags":[],"cell_id":"0ee0554e-8e93-4025-8027-99b614633380"},"outputs":[],"execution_count":66},{"cell_type":"code","source":"module = nn.LSTM(5, 2)\ndp_module = WeightDropout(module, 0.4)\ngetattr(dp_module.module, WEIGHT_HH)","metadata":{"tags":[],"cell_id":"b397c105-c917-486d-8ecb-e1f43aba87d7"},"outputs":[{"output_type":"execute_result","execution_count":67,"data":{"text/plain":"Parameter containing:\ntensor([[ 0.3743,  0.5702],\n        [-0.4095, -0.1545],\n        [ 0.6281, -0.4933],\n        [-0.4411, -0.5618],\n        [-0.3762, -0.5055],\n        [-0.1844,  0.6921],\n        [-0.6187,  0.1410],\n        [-0.1217, -0.4306]], requires_grad=True)"},"metadata":{}}],"execution_count":67},{"cell_type":"code","source":"#export\nclass EmbeddingDropout(nn.Module):\n    \"Applies dropout in the embedding layer by zeroing out some elements of the embedding vector.\"\n    def __init__(self, emb, embed_p):\n        super().__init__()\n        self.emb, self.embed_p = emb,embed_p\n        self.pad_idx = self.emb.padding_idx\n        if self.pad_idx is None: self.pad_idx = -1\n\n    def forward(self, words, scale=None):\n        if self.training and self.embed_p != 0:\n            size = (self.emb.weight.size(0),1)\n            mask = dropout_mask(self.emb.weight.data, size, self.embed_p)\n            masked_embed = self.emb.weight * mask\n        else: masked_embed = self.emb.weight\n        if scale: masked_embed.mul_(scale)\n        return F.embedding(words, masked_embed, self.pad_idx, self.emb.max_norm,\n                           self.emb.norm_type, self.emb.scale_grad_by_freq, self.emb.sparse)","metadata":{"tags":[],"cell_id":"012e1f45-1a3d-439e-bd36-25f57bfae009"},"outputs":[],"execution_count":68},{"cell_type":"code","source":"enc = nn.Embedding(100, 7, padding_idx=1)\nenc_dp = EmbeddingDropout(enc, 0.5)\ntst_input = torch.randint(0,100,(8,))\nenc_dp(tst_input)","metadata":{"tags":[],"cell_id":"bd5d081c-fa99-42ed-9730-1e2ef074f1d2"},"outputs":[{"output_type":"execute_result","execution_count":69,"data":{"text/plain":"tensor([[ 4.6001,  1.0425,  3.3220,  0.2483, -0.1753,  1.9429,  0.8193],\n        [ 0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [-0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n        [ 1.4824, -2.0635, -0.9413,  2.2616, -0.4480, -1.7999,  1.3532],\n        [ 0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000],\n        [-0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000],\n        [-1.5572, -1.3358,  1.5200,  0.4026, -2.5249, -3.1842, -2.6833],\n        [ 0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000]],\n       grad_fn=<EmbeddingBackward>)"},"metadata":{}}],"execution_count":69},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"ec6488a5-e09c-41ec-9180-126009c7bef5"},"source":"## Main Model"},{"cell_type":"markdown","source":"The main model is a regular LSTM with several layers, but using all those kinds of dropouts.","metadata":{"tags":[],"cell_id":"88972762-1188-43d3-a938-1108caca437a"}},{"cell_type":"code","source":"#export\ndef to_detach(h):\n    \"Detaches `h` from its history.\"\n    return h.detach() if type(h) == torch.Tensor else tuple(to_detach(v) for v in h)","metadata":{"tags":[],"cell_id":"f78491d4-88d3-441d-b393-4376599a68fa"},"outputs":[],"execution_count":70},{"cell_type":"code","source":"","metadata":{"tags":[],"cell_id":"952bc877-7595-4d15-9971-4c4523ffd259"},"outputs":[],"execution_count":null}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"f239f01d-f1d4-4a55-b37d-6313432feec1","deepnote_execution_queue":[]}}